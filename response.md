# Question 1

This might be kinda unfun to deal with. I'll try to posit a few ideas:

Firstly, just don't. Avoid this situation entirely. Use two transformers with the same tokens.

Secondly, maybe your amateur's tokens are a subset of your expert's. So just use your amateur's tokenization. Though this does seem slightly fake; if for example " bucket" was an expert token but the amateur tokens are " buck" and "et", you might get a situation where an input sentence like "She built a sandcastle using a plastic" would not have the expert predicting " buck" very strongly because its instead predicting " bucket". So that'd be a bit sad.

But now that I've elaborated on the problem I perhaps also see how to fix it: use the expert to get probabilities for every token, then for each token that the expert has that the amateur does not, add its probability to the token the amateur would use instead. That was a mouthful so let's talk about an example. Suppose the expert predicts some logits for the tokens " water", " watermelon", " watercolor", and " waterski", but the only one of these tokens the amateur has is " water". Take the logsumexp of all the expert logits, and then compare them to the amateur logit for " water". Might be interesting!

Ok but unfortunately there's still a clear issue with this which is that if the expert generates " water", the next token generated by the expert might be predicted to begin with a space because of how the token *wasn't* any of the words beginning with water.

This inspires the idea in the specific case that you're generating space-separated words from a finite dictionary that you can just compute the probability of each word being the next word, as tokenizers (at least, tokenizers that make me happy) will at least agree that spaces will split up tokens so that tokens don't span multiple words.

Or again with the assumption that tokens don't span multiple words, another idea is to do the logsumexp water thing except instead of feeding " water" into the expert if it's selected, just feeding it into the amateur and restricting the output to one of "melon", "color", "ski", and a token beginning with a space.

Another idea is to use transformers that were trained on somewhat random tokenizations of the same sentence. So that they don't learn weird rules such as not predicting the token "color" after the token " water" because there's already a token for " watercolor".

Oh and of course, there's always fine-tuning them to use only their shared tokenizers. But that'd take data and compute. So how good of an idea this is massively depends on whether you have data and compute (and how large the models you want to use are).

To be honest whatever works works and it's hard to tell without trying myself. There's a decent chance that right off the bat it doesn't need to get this messy and that the transformers would be fine as they are.

# Question 2

Instead of answering this directly, I'll first ask myself if I would think it *should* be used in practice.

Hm. So firstly, one thought that popped up while coding line 73 was that I would be skeptical of how frequently this method generates EOS tokens, as I would think that those would be generally well-predicted by the amateur, and not significantly less well-predicted than the expert's predictions would be. You would need the expert to be so confident that a token is an EOS token that the EOS token is the only thing that passes through the plausibility constraint. And I dunno. I mean, on the one hand, it only requires the EOS token being 10x more likely than other tokens, but on the other, spaces and linebreaks should be pretty common alternatives to an EOS token.

In terms of the efficiency of the algorithm itself, I do kinda like the spirit of it. Passing an input through a small transformer should be pretty fast; I'd think it wouldn't add too much inference time unless I'm missing something.

Side note: one idea I had earlier was that instead of taking "expert_logits - amateur_logits" and using the plausibility constraint, one could perhaps combine the effects of both at once by doing something like expert_logits - (amateur_logits / 2). So if the expert calls something -15 and the amateur -20, that still comes out to -5 not 5, which is a good sanity check, as would otherwise be the purpose of the plausibility constraint. Side note on this side note: to be clear, the reason doing this subtraction is equivalent is Bayes' rule.

Oh another note: I think a great reason I'd use it in practice is that in theory, if this does what the paper suggests it does (I'd need to see more outputs to know my opinion more clearly), that's just cool. Like, it'd just generate particularly interesting outputs on purpose without you needing to do anything. Outputs that specifically were learned by a larger model but *not* by a smaller model. Which is simply just a win, in theory.

So would it be used in pratice? Honestly I still won't have a strong read on this until I see more. And if I were using in practice, I feel like I'd wanna make a heckuva lotta adjustments to the idea because I'm not too keen on its form as proposed in the paper. But IMO, it seems like this idea has a ton of potential.

OH OH OH actually compared to the alternative of beam search, this idea seems particularly great; it'd take so much less compute to get creativity. HUGE win. I lowkey kinda see using this method with sufficient modifications. Model might just seem so much more creative and intellegent. Still have to see more of its outputs for myself but yoooooo that's epic.